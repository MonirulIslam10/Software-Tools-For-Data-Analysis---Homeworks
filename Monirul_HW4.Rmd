---
title: "Homework 4"
author: "Monirul Islam"
date: "2024-05-02"
output:
  html_document: default
  pdf_document: default
---
## Question 1
```{r include=FALSE}
library(tidyverse)
```

### (Q1a) Find the maximum element of a vector (only using loops, conditionals, and Boolean operators - no max, min, sort or similar).
```{r}
mymax <-function(vector) {
  max_element = vector[1]
  for (element in length(vector)) {
    if (vector[element] > max_element) {
      max_element = vector[element]
    }
  }
  print(max_element)
}
```
```{r}
mymax(c(3, 4, 5))
mymax(c(-1, 0, 0, 4))
mymax(c(3, 2, 3))
```

### (Q1b) State the current date and time:
```{r}
when <- function() {
  time = strftime(Sys.time(), "%I:%M%P on %A, %B %dth, %Y")
  print(paste('It is',paste0(time,'.')))
}
```
```{r}
when()
```

### (Q1c) Make random ‘Secret Santa’ assignments: that is, take a list of n elements, and randomly map each element to another element without mapping any element to itself.
```{r}
secret_santa <-function(n) {
  repeat {
    random = sample(n) 
    if (all(!(n == random))) {
      break 
    }
  }
  secret_santa_table <- data.frame(Giver = n, Recipient = random)
  print(secret_santa_table)
}
```
```{r}
secret_santa(c("Michael", "Gus", "Elizabeth", "Mary"))
```

### (Q1d) Find the longest string in a character vector: the nchar function may be helpful here.
```{r}
longest_str <- function(vector) {
  max_length = max(nchar(vector))
  print(vector[nchar(vector) == max_length])
}
```
```{r}
longest_str(c("Elizabeth", "Michael", "Mary", "Gus"))
longest_str(c("Baruch", "Hunter", "John Jay"))
```

### (Q1e) Convert a temperature in Fahrenheit to Celsius:
```{r}
f2c <- function(f) {
  c = (5/9)*(f-32)
  print(c)
}
```
```{r}
f2c(32)
f2c(212)
f2c(-40)

```

## Question 2
```{r include=FALSE}
library(tidyverse)
library(babynames)
```
``` {r}
data(babynames)
glimpse(babynames)
```

### (Q2a) In how many different years was ‘Michael’ the most common boys’ name?
```{r}
Michael <- babynames |> group_by(year) |> filter(sex == "M", n == max(n), name == "Michael") 
Michael
```
44 years where "Michael" was the most common boys name.

### (Q2b) In what year was the first child named ‘Daddy’ born?
```{r}
Daddy <- babynames |> filter(name == "Daddy") |> arrange(year) |> head(n=1)
Daddy
```

### (Q2c) In what year, did the top 5 girls names comprise the highest fraction of total births?
```{r}
total_births <- babynames |> 
  group_by(year) |> 
  summarise(total_births = sum(births)) 

top_five_girlsNames <- babynames |> 
  filter(sex == 'F') |>
  group_by(year) |>
  arrange(desc(n)) |> 
  slice_max(n, n=5,with_ties = FALSE) |>
  summarise(top_5 = sum(n))

top5_totalBirths <- inner_join(top_five_girlsNames,total_births,join_by(year)) |>
  mutate(frac = top_5/total_births) |>
  arrange(desc(frac)) |>
  head(n=1)
top5_totalBirths
```

### (Q2d) What name has been given to more girls than any other?
```{r}
girl <- babynames |> filter(sex == "F") |> 
  group_by(name) |> 
  summarise(total = sum(n)) |> 
  arrange(desc(total)) |> 
  head(n=1)
girl
```

### (Q2e) In what decade were the most ”Adolph”s born in the US? (You will need to use a mutate to create a new decade variable.)
```{r}
Adolph <- babynames |> 
  filter(name == "Adolph") |>
  mutate(decade = 10 * (year %/% 10)) |>
  group_by(decade) |>
  summarise(total = sum(n)) |>
  arrange(desc(total)) |>
  head(n=1)
Adolph
```

## Question 3
```{r include=FALSE}
library(tidyverse)
library(tidyr)
library(ggplot2)
library(CVXR)
```
``` {r}
data(cdiac)
glimpse(cdiac)
```

### (Q3a) Plot the estimated annual global mean temperature (GMT) anomaly from 1850 to 2015.
```{r}
ggplot(cdiac, aes(x=year,y=annual)) + 
  geom_point() + 
  theme_bw() +
  ggtitle("Annual Global Mean Temperature (GMT) Anomaly from 1850 to 2015") +
  xlab("Year") +
  ylab("GMT Anomaly")
```

### (Q3b) Plot the GMT anomaly for each month on the same plot (as different lines).
```{r}
cdiac_longer <- cdiac |>
  pivot_longer(cols = "jan":"dec", names_to = "Month", values_to = "GMT") |>
  select(-annual) 
ggplot(cdiac_longer, aes(x=year, y=GMT, color=Month)) +
  geom_line() +
  theme_bw() +
  ggtitle("Monthly Global Mean Temperature (GMT) Anomaly from 1850 to 2015") +
  xlab("Year") +
  ylab("GMT Anomaly") +
  theme(legend.position = 'bottom') +
  theme(legend.key.width = unit(.5,'cm')) 
```

### (Q3c) Plot the monthly GMT anomaly series as one long line (with a point for each month).
```{r}
cdiac_longer_formatted <- cdiac_longer |> 
  mutate(Date = as.Date(paste(Month, "1", year, sep = "-"), format = "%b-%d-%Y")) |>
  select(Date, GMT)

ggplot(cdiac_longer_formatted, aes(x = Date, y = GMT)) +
  geom_point() +
  geom_line(alpha=0.5) +
  ggtitle("Monthly Global Mean Temperature (GMT) Anomaly from 1850 to 2015") +
  ylab("GMT Anomaly")

```

### (Q3d) Now focus only on July: plot the July GMT anomaly series. Use the runmed function to add a second series to the plot giving the median July GMT anomaly of the previous 5 years. Is there evidence of an increasing warming trend?
```{r}
July <- cdiac |> select(year,jul) |> mutate(median_jul = runmed(jul, k=5))
ggplot(July, aes(x=year)) + 
  geom_line(aes(y=jul)) + 
  geom_line(aes(y=median_jul,color="red")) +
  xlab("Year") + 
  ylab("GMT Anomaly") +
  theme_bw() +
  theme(legend.position = 'none') +
  ggtitle("July GMT with Running Median") 
```

The lines following the same direction can be evidence of an increasing warming trend. 

### (Q3e) For each year, identify the warmest month (as measured by GMT anomaly); create a histogram showing the probability a given month was the hottest in its year. Make sure your x-axis is in reasonable (chronological) order - not alphabetical.
```{r}
warmest_months <- cdiac_longer |> 
  group_by(year) |> 
  filter(GMT == max(GMT)) 
warmest_months$Month = factor(warmest_months$Month, 
levels = c("jan","feb","mar","apr", "may","jun","jul","aug","sep","oct","nov","dec"))

ggplot(warmest_months , aes(x=Month)) +
  geom_histogram(stat = 'count') + 
  ylab("Frequency") +
  scale_x_discrete(limits = levels(warmest_months$Month)) 
```

## Question 4
```{r include=FALSE}
library(tidyverse)
library(stringr)
```

### (Q4a) In the following sentence, extract all plural nouns
```{r}
todo1 <- "Yesterday, I needed to buy four cups of flour, a piece of Parmesean cheese,
two gallons of ice cream, and a six-pack of bottled (non-alcoholic) beers."
```
```{r}
str_extract_all(todo1, "\\b\\w+s\\b")
```

### (Q4b) In the following sentence, compute the total number of fruits on my shopping list:
```{r}
todo2 <- "Today, I need to purchase 3 apples, 5 limes, and 2 lemons."
```
```{r}
digits <- str_extract_all(todo2, "\\b[:digit:]\\b")
sum(as.numeric(unlist(digits)))
```

### (Q4c) In the first page of her Wikipedia page, how many times does Taylor Swift’s last name appear?
```{r}
taylor <- "Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter.
A subject of widespread public interest, she has influenced the music industry
and popular culture through her artistry, especially in songwriting, and entrepreneurship.
She is an advocate of artists rights and womens empowerment. Swift began professional
songwriting at age 14. She signed with Big Machine Records in 2005 and achieved
prominence as a country pop singer with the albums Taylor Swift (2006) and Fearless
(2008). Their singles ’Teardrops on My Guitar’, ’Love Story’, and ’You Belong
with Me’ were crossover successes on country and pop radio formats and brought
Swift mainstream fame. She experimented with rock and electronic styles on her
next albums, Speak Now (2010) and Red (2012), respectively, with the latter featuring
her first Billboard Hot 100 number-one single, ’We Are Never Ever Getting Back
Together’. Swift recalibrated her image from country to pop with 1989 (2014),
a synth-pop album containing the chart-topping songs ’Shake It Off’, ’Blank Space’,
and ’Bad Blood’. Media scrutiny inspired the hip-hop-influenced Reputation (2017)
and its number-one single ’Look What You Made Me Do’. After signing with Republic
Records in 2018, Swift released the eclectic pop album Lover (2019) and the autobiographical
documentary Miss Americana (2020). She explored indie folk styles on the 2020
albums Folklore and Evermore, subdued electropop on Midnights (2022), and re-recorded
four albums subtitled Taylors Version after a dispute with Big Machine. These
albums spawned the number-one songs ’Cruel Summer’, ’Cardigan’, ’Willow’, ’Anti-Hero’,
’All Too Well’, and ’Is It Over Now?’. Her Eras Tour (2023-2024) and its accompanying
concert film became the highest-grossing tour and concert film of all time, respectively.
Swift has directed videos and films such as Folklore: The Long Pond Studio Sessions
(2020) and All Too Well: The Short Film (2021). Swift is one of the worlds best-selling
artists, with 200 million records sold worldwide as of 2019. She is the most-streamed
artist on Spotify, the highest-grossing female touring act, and the first billionaire
with music as the main source of income. Six of her albums have opened with over
one million sales in a week. The 2023 Time Person of the Year, Swift has appeared
on lists such as Rolling Stones 100 Greatest Songwriters of All Time, Billboards
Greatest of All Time Artists, and Forbes Worlds 100 Most Powerful Women. Her
accolades include 14 Grammy Awards, a Primetime Emmy Award, 40 American Music
Awards, 39 Billboard Music Awards, and 23 MTV Video Music Awards; she has won
the Grammy Award for Album of the Year, the MTV Video Music Award for Video of
the Year, and the IFPI Global Recording Artist of the Year a record four times
each."
```
```{r}
str_count(taylor, "Swift")
```

### (Q4d) In the above quote, how many different years (strings of exactly 4 digits) appear?
```{r}
unique(unlist(str_extract_all(taylor, "\\b\\d{4}\\b")))
```

### (Q4e) Extract the names of all songs mentioned in the biography above. (Note that song names are surrounded by single quotes.)
```{r}
str_extract_all(taylor,'’(.*?)’')
```

## Question 5
```{r include=FALSE}
library(tidyverse)
library(rvest)
library(httr)
library(stringr)
```

### (Q5a) How many quotes are on this website?
```{r}
BASE_URL <- "https://quotes.toscrape.com/"
BASE_HTML <- read_html("https://quotes.toscrape.com/")
all_quotes_website <- function(BASE_HTML) {
  page_number = 1
  all_quotes = character()
  while (TRUE) {
    all_URLs <- read_html(paste0(BASE_HTML, "page/", page_number))
    quotes <- all_URLs |> 
              html_elements(".quote") |>                                        
              html_text(trim = TRUE) 
    if (length(quotes) == 0) {
      break
    }
    all_quotes = c(all_quotes,quotes)
    page_number = page_number + 1
  }
  all_quotes
}
```
```{r}
length(all_quotes_website(BASE_URL))
```

### (Q5b) How many quotes are tagged ‘Death’?
```{r}
all_tags_website <- function(BASE_HTML) {
  page_number = 1
  all_tags = character()
  while (TRUE) {
    all_URLs <- read_html(paste0(BASE_HTML, "page/", page_number))
    tags <- all_URLs |> 
      html_elements(".tags") |>  
      html_text(trim = TRUE) 
    if (length(tags) == 0) {
      break
    }
    all_tags = c(all_tags,tags)
    page_number = page_number + 1
  }
  all_tags
}
```
```{r}
sum(str_detect(all_tags_website(BASE_URL), "death\n"))
```

### (Q5c) What is the longest quote (by number of characters)?
```{r}
all_text_website <- function(BASE_HTML) {
  page_number = 1
  all_texts = character()
  while (TRUE) {
    all_URLs <- read_html(paste0(BASE_HTML, "page/", page_number))
    texts <- all_URLs |> 
      html_elements(".text") |>  
      html_text(trim = TRUE) 
    if (length(texts) == 0) {
      break
    }
    all_texts = c(all_texts,texts)
    page_number = page_number + 1
  }
  all_texts
}
```
```{r}
which.max(nchar(all_text_website(BASE_URL)))
max(nchar(all_text_website(BASE_URL)))
```

### (Q5d) How many quotes are by (or at least are attributed to) Albert Einstein?
```{r}
all_author_website <- function(BASE_HTML) {
  page_number = 1
  all_authors = character()
  while (TRUE) {
    all_URLs <- read_html(paste0(BASE_HTML, "page/", page_number))
    authors <- all_URLs |> 
      html_elements(".author") |>  
      html_text(trim = TRUE) 
    if (length(authors) == 0) {
      break
    }
    all_authors = c(all_authors,authors)
    page_number = page_number + 1
  }
  all_authors
}
```
```{r}
sum(str_detect(all_author_website(BASE_URL), "Albert Einstein"))
```

## Question 6 
```{r include=FALSE}
library(tidyverse)
library(infer)
authors <- read.csv("~/Desktop/Weylandt HW/Weylandt HW4/authors_all.csv")
glimpse(authors)
```

### (Q6a) Jack London uses the word 'should' an average of three or more times per chapter.
```{r}
London_should <- authors |> filter(Author == "London") |> summarise(avg_should = mean(should))
London_should


London_should_table <- authors |> filter(Author == "London") |> select(should,Author)
boot_London_should_median <- replicate(1000,{
  n_rows <- NROW(London_should_table)
  London_should_table |>
    sample_n(n_rows, replace = TRUE) |>
    summarize(median_should = median(should)) |>
    pull(median_should) 
})
hist(boot_London_should_median)
```

According to the mean and bootstrap model with median, London does not use the word 'should' on average three times or more. 

### (Q6b) Jane Austen uses the word 'her' more than any of the other authors. 
```{r}
her_table <- authors |> 
  select(her, Author) |> 
  mutate(Author = case_when(Author == "Austen" ~ Author, TRUE ~ "Other"))
t.test(her~Author, data = her_table)


boot_her_diff_mean <- replicate(1000,{
  n_rows <- NROW(her_table)
  her_table |>
    sample_n(n_rows, replace = TRUE) |>
    group_by(Author) |>
    summarize(mean_her = mean(her)) |>
    arrange(desc(Author)) |>
    pull(mean_her) |>
    diff()
})
hist(boot_her_diff_mean)
sd(boot_her_diff_mean)


boot_her_diff_median <- replicate(1000,{
  n_rows <- NROW(her_table)
  her_table |>
    sample_n(n_rows, replace = TRUE) |>
    group_by(Author) |>
    summarize(median_her = median(her)) |>
    arrange(desc(Author)) |>
    pull(median_her) |>
    diff()
})
hist(boot_her_diff_median)
```

The t-test and bootstrap models support the hypothesis that Jane Austen uses 'her' more than any other author. Austen's average use of 'her' is about 30 while the other authors average about 7. 
The small p-value supports the alternative hypothesis that the true difference in means between group Austen and group Other (other authors) is not equal to 0. 

### (Q6c) Jack London and John Milton use the word 'also' the same amount.
```{r}
London_Milton_also <- authors |> filter(Author %in% c("London","Milton")) |> select(also,Author)
t.test(also ~ Author, data = London_Milton_also)


boot_also_diff_mean <- replicate(1000,{
  n_rows <- NROW(London_Milton_also)
  London_Milton_also |>
    sample_n(n_rows, replace = TRUE) |>
    group_by(Author) |>
    summarize(mean_also = mean(also)) |>
    arrange(desc(Author)) |>
    pull(mean_also) |>
    diff()
})
hist(boot_also_diff_mean)
sd(boot_also_diff_mean)


boot_also_diff_median <- replicate(1000,{
  n_rows <- NROW(London_Milton_also)
  London_Milton_also |>
    sample_n(n_rows, replace = TRUE) |>
    group_by(Author) |>
    summarize(median_also = median(also)) |>
    arrange(desc(Author)) |>
    pull(median_also) |>
    diff()
})
hist(boot_also_diff_median)
```

A high p-value and bootstrap models show we can't entirely reject the null hypothesis as there could be times where the usage of 'also' between London and Milton could be the same or different. The difference in means is about 0.09 with a standard deviation of 0.13.  

### (Q6d) The word 'must' is correlated with the word 'should'.
```{r}
cor.test(authors$must,authors$should)


ggplot(authors, aes(x=must,y=should)) + geom_point()
```

The correlation value being close to 0 and the plot shows that there is not a meaningful relationship between 'must' and 'should'.

### (Q6e) Jane Austen uses the words 'a' and 'the' at the same rate.
```{r}
Austen_a_the <- authors |> filter(Author == "Austen") |> select(a,the, Author) 
t.test(Austen_a_the$a,Austen_a_the$the)


Austen_a <- authors |> filter(Author == "Austen") |> select(a,Author)
boot_Austen_a_mean <- replicate(1000,{
  n_rows <- NROW(Austen_a)
  Austen_a |>
    sample_n(n_rows, replace = TRUE) |>
    summarize(mean_a = mean(a)) |>
    pull(mean_a) 
})
hist(boot_Austen_a_mean)

boot_Austen_a_median <- replicate(1000,{
  n_rows <- NROW(Austen_a)
  Austen_a |>
    sample_n(n_rows, replace = TRUE) |>
    summarize(median_a = median(a)) |>
    pull(median_a) 
})
hist(boot_Austen_a_median)


Austen_the <- authors |> filter(Author == "Austen") |> select(the, Author) 
boot_Austen_the_mean <- replicate(1000,{
  n_rows <- NROW(Austen_the)
  Austen_the |>
    sample_n(n_rows, replace = TRUE) |>
    summarize(mean_the = mean(the)) |>
    pull(mean_the) 
})
hist(boot_Austen_the_mean)

boot_Austen_the_median <- replicate(1000,{
  n_rows <- NROW(Austen_the)
  Austen_the |>
    sample_n(n_rows, replace = TRUE) |>
    summarize(median_the = median(the)) |>
    pull(median_the) 
})
hist(boot_Austen_the_median)
```

The t-test and bootstrap models show that Austen uses 'a' on average 31 times and uses 'the' on average 61 times. The small p-value supports the alternative hypothesis that the true difference in means is not equal to 0. Therefore, Austen does not use the words 'a' and 'the' at the same rate. 

## Question 7
```{r include=FALSE}
library(tidymodels)
library(tidyverse)
library(ranger)
library(glmnet)
library(rpart)
library(readxl)
library(GGally)
authors <- read.csv("~/Desktop/Weylandt HW/Weylandt HW4/authors_all.csv") |> mutate(Author = factor(Author))
```

### • Perform a test/train split of the data.
```{r}
DATA_split = initial_split(authors, prop = 0.8)
DATA_training = training(DATA_split)
Data_testing = testing(DATA_split)
```

### • Fit K-Nearest Neighbors with the author identity as the response.
```{r}
knn_spec <- nearest_neighbor(neighbors = 5, weight_func = 'triangular') |>
            set_mode("classification") |>
            set_engine("kknn")

wflow_knn <- workflow() |>
             add_formula(Author ~ .) |>
             add_model(knn_spec)

predictor_knn <- fit(wflow_knn,data=DATA_training)
predictor_knn
```

### • Use hyperparameter tuning to identify the best value of K.
```{r}
knn_spec_tuning <- nearest_neighbor(neighbors = tune()) |>
                   set_mode("classification") |>
                   set_engine("kknn")

knn_grid <- grid_regular(neighbors(), levels = 5)

wflow_knn_tuning <- workflow() |>
  add_formula(Author ~ .) |>
  add_model(knn_spec_tuning)

author_cv <- vfold_cv(DATA_training, v=5) 

tuning_results <- wflow_knn_tuning |>
                  tune_grid(resamples = author_cv, grid = knn_grid)

best_knn <- tuning_results |> select_best(metric = "accuracy")
predictor_best_knn <- wflow_knn_tuning |> finalize_workflow(best_knn) |> fit(DATA_training)
predictor_best_knn
```

### • Compute and report classification accuracy on both the training and test sets.
```{r}
#Accuracy test Training Data 
augment(predictor_best_knn,DATA_training) |>
  select(Author,.pred_class) |>
  accuracy(estimate=.pred_class,truth=Author)


#Accuracy test Testing Data 
augment(predictor_best_knn,Data_testing) |>
  select(Author,.pred_class) |> 
  accuracy(estimate=.pred_class,truth=Author)
```


## Question R

### What have you learned in this course that you did not anticipate learning?
Before taking this course, I was only familiar with the idea that R is a programming language with vast capabilities. Throughout the semester I was introduced to several topics and packages in R that can be utilized. I learned to create functions with control flow, import and manipulate data, create different types of visualizations to present data, do web-scraping with string manipulation, test hypotheses with statistical analysis, and do some predictive modeling. I plan to continue practicing these topics to become more efficient in applying the concepts to new questions and datasets.           

### What skills that you learned in this course do you anticipate will be the most helpful in future classes? In your future career?
Some skills I believe will be helpful in the future include importing both structured and unstructured data, cleaning, and normalizing to perform different statistical tests and other data manipulation analyses. In giving a presentation to showcase the results, producing the appropriate visualizations is important for audience consumption. 

### What skills or techniques do you want to learn more about?
I want to continue learning about predictive modeling techniques with machine learning as it is still a fairly new topic of high importance. Forecasting future metrics and testing hypothetical instances to datasets can help any company improve its operations, navigate contingencies,  and stay ahead of the competition. Specific topics I am interested in enhancing predictive modeling include learning different ensemble methods that combine predictions from different models to improve accuracy, time series forecasting, and model evaluation. 

### If one topic could be added to this course, what would it be?
Maybe to build off the topic of visualizations, something to add is learning to make interactive ones that can be exported as a dashboard. Including more statistical modeling like survival analysis, multivariate analysis, and time series can be beneficial for data analysis. 

### How could you demonstrate mastery of the skills learned in this course during an interview?
To demonstrate the mastery of skills learned in this course during an interview, I would discuss the skills used in producing the final project. This includes the motivation behind the underlying questions proposed, identifying different types of data sources, importing, cleaning, and manipulating the datasets to perform analysis, and creating a variety of visualizations to portray the results and tell a story to answer the proposed questions. Aside from the project, I would also discuss particular homework questions that utilized a specific skill learned throughout the course, such as question 5, where we had to create a function to sift through the web pages and extract specific information to manipulate further.  






